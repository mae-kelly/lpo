#!/bin/bash

echo "ğŸš€ Deploying MASSIVE LBO Data Pipeline..."
echo "ğŸ¯ Target: 10,000+ real LBO transactions from all free sources!"

# Install dependencies
echo "ğŸ“¦ Installing comprehensive dependencies..."
python3 -m pip install requests beautifulsoup4 pandas numpy feedparser python-dateutil
python3 -m pip install scikit-learn sentence-transformers newspaper3k selenium
python3 -m pip install aiohttp asyncio tqdm webdriver-manager
python3 -m pip install spacy nltk fastapi uvicorn pydantic

# Download spaCy model
echo "ğŸ§  Downloading NLP model..."
python3 -m spacy download en_core_web_sm || echo "SpaCy model download failed - continuing anyway"

# Run the massive scraping pipeline
echo "ğŸ” Starting MASSIVE data scraping pipeline..."
python3 massive_lbo_scraper.py

# Train ML models on massive dataset
echo "ğŸ§  Training ML models on massive dataset..."
python3 ml_trainer_massive.py

echo "âœ… MASSIVE LBO Pipeline deployment complete!"
echo ""
echo "ğŸ“Š Check these files for results:"
echo "  â€¢ massive_lbo_report.txt - Comprehensive data report"
echo "  â€¢ massive_lbo_database.db - SQLite database with all deals"
echo "  â€¢ massive_lbo_models.pkl - Trained ML models"
echo ""
echo "ğŸ¯ Data sources scraped:"
echo "  â€¢ 16 major PE firm portfolio pages"
echo "  â€¢ Business Wire press releases"
echo "  â€¢ Wikipedia LBO lists"
echo "  â€¢ SEC EDGAR filings"
echo ""
echo "ğŸ’¾ Expected dataset size: 5,000-15,000 real LBO transactions"
